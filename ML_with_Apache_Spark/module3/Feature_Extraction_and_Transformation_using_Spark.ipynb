{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Transformation using Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color: red'>The purpose of this lab is to show you how to use Spark to extract and transform features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "    <a href=\"#Objectives\">Objectives\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Datasets\">Datasets\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Setup\">Setup\n",
    "    </a>\n",
    "    <ol>\n",
    "      <li>\n",
    "        <a href=\"#Installing-Required-Libraries\">Installing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "      <li>\n",
    "        <a href=\"#Importing-Required-Libraries\">Importing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li> \n",
    "    <a href=\"#Examples\">Examples\n",
    "    </a>\n",
    "    <ol>\n",
    "    <li>\n",
    "      <a href=\"#Task-1---Tokenizer\">Task 1 - Tokenizer\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-2---CountVectorizer\">Task 2 - CountVectorizer\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-3---TF-IDF\">Task 3 - TF-IDF\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-4---StopWordsRemover\">Task 4 - StopWordsRemover\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-5---StringIndexer\">Task 5 - StringIndexer\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-6---StandardScaler\">Task 6 - StandardScaler\n",
    "      </a>\n",
    "    </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Exercises\">Exercises\n",
    "    </a>\n",
    "  </li>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-1---Tokenizer\">Exercise 1 - Tokenizer\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-2---CountVectorizer\">Exercise 2 - CountVectorizer\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-3---StringIndexer\">Exercise 3 - StringIndexer\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-4---StandardScaler\">Exercise 4 - StandardScaler\n",
    "      </a>\n",
    "    </li>\n",
    "  </ol>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Use the feature extractor CountVectorizer\n",
    " - Use the feature extractor TF-IDF\n",
    " - Use the feature transformer Tokenizer\n",
    " - Use the feature transformer StopWordsRemover\n",
    " - Use the feature transformer StringIndexer\n",
    " - Use the feature transformer StandardScaler\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In this lab you will be using dataset(s):\n",
    "\n",
    " - Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) for connecting to the Spark Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.\n",
    "\n",
    "If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned <a href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/labs/Connecting_to_spark_cluster_using_Skills_Network_labs.ipynb\">here.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 16:49:44 WARN Utils: Your hostname, codebase resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/08/29 16:49:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 16:49:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is used to break a sentence into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenizer\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (1, \"Spark is a distributed computing system.\"),\n",
    "    (2, \"It provides interfaces for multiple languages\"),\n",
    "    (3, \"Spark is built on top of Hadoop\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |sentence                                     |\n",
      "+---+---------------------------------------------+\n",
      "|1  |Spark is a distributed computing system.     |\n",
      "|2  |It provides interfaces for multiple languages|\n",
      "|3  |Spark is built on top of Hadoop              |\n",
      "+---+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the dataframe\n",
    "sentenceDataFrame.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer instance.\n",
    "#mention the column to be tokenized as inputcol\n",
    "#mention the output column name where the tokens are to be stored.\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "token_df = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                     |words                                               |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |\n",
      "|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n",
      "|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n",
      "+---+---------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the tokenized data\n",
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer is used to convert text into numerical format. It gives the count of each word in a given document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+\n",
      "|id |words                                            |\n",
      "+---+-------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|\n",
      "+---+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a sample dataframe and display it.\n",
    "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
    "            (2, \"I love Python Spark supports Python\".split()),\n",
    "            (3, \"Spark solves the big problem of big data\".split())]\n",
    "\n",
    "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
    "\n",
    "textdata.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "# mention the column to be count vectorized as inputcol\n",
    "# mention the output column name where the count vectors are to be stored.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the CountVectorizer model on the input data\n",
    "model = cv.fit(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input data to bag-of-words vectors\n",
    "result = model.transform(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+---------------------------------------------------+\n",
      "|id |words                                            |features                                           |\n",
      "+---+-------------------------------------------------+---------------------------------------------------+\n",
      "|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,4,10,11],[2.0,1.0,1.0,1.0,1.0,1.0])     |\n",
      "|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,4,8],[1.0,2.0,1.0,1.0,1.0])             |\n",
      "|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,3,5,6,7,9,12],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+---+-------------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the dataframe\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency is used to quantify the importance of a word in a document. TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary classes for TF-IDF calculation\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|id |sentence     |\n",
      "+---+-------------+\n",
      "|1  |good boy     |\n",
      "|2  |good girl    |\n",
      "|3  |boy good girl|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a sample dataframe and display it.\n",
    "sentenceData = spark.createDataFrame([\n",
    "        (1, \"good boy\"),\n",
    "        (2, \"good girl\"),\n",
    "        (3, \"boy good girl\")\n",
    "    ], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------------+\n",
      "|id |sentence     |words            |\n",
      "+---+-------------+-----------------+\n",
      "|1  |good boy     |[good, boy]      |\n",
      "|2  |good girl    |[good, girl]     |\n",
      "|3  |boy good girl|[boy, good, girl]|\n",
      "+---+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenize the \"sentence\" column and store in the column \"words\"\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------------+--------------------------+\n",
      "|id |sentence     |words            |rawFeatures               |\n",
      "+---+-------------+-----------------+--------------------------+\n",
      "|1  |good boy     |[good, boy]      |(10,[7,8],[1.0,1.0])      |\n",
      "|2  |good girl    |[good, girl]     |(10,[8,9],[1.0,1.0])      |\n",
      "|3  |boy good girl|[boy, good, girl]|(10,[7,8,9],[1.0,1.0,1.0])|\n",
      "+---+-------------+-----------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a HashingTF object\n",
    "# mention the \"words\" column as input\n",
    "# mention the \"rawFeatures\" column as output\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IDF object\n",
    "# mention the \"rawFeatures\" column as input\n",
    "# mention the \"features\" column as output\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------+\n",
      "|sentence     |features                                                  |\n",
      "+-------------+----------------------------------------------------------+\n",
      "|good boy     |(10,[7,8],[0.28768207245178085,0.0])                      |\n",
      "|good girl    |(10,[8,9],[0.0,0.28768207245178085])                      |\n",
      "|boy good girl|(10,[7,8,9],[0.28768207245178085,0.0,0.28768207245178085])|\n",
      "+-------------+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the tf-idf data\n",
    "tfidfData.select(\"sentence\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StopWordsRemover is a transformer that filters out stop words like \"a\",\"an\" and \"the\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StopWordsRemover\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+\n",
      "|id |sentence                                                    |\n",
      "+---+------------------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n",
      "+---+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe with sample text and display it\n",
    "textData = spark.createDataFrame([\n",
    "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
    "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
    "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n",
    "textData = remover.transform(textData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|id |sentence                                                    |filtered_sentence                                   |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n",
      "|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n",
      "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n",
      "+---+------------------------------------------------------------+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the dataframe\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - StringIndexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer converts a column of strings into a column of integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| color|\n",
      "+---+------+\n",
      "|  0|   red|\n",
      "|  1|   red|\n",
      "|  2|  blue|\n",
      "|  3|yellow|\n",
      "|  4|yellow|\n",
      "|  5|yellow|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe with sample text and display it\n",
    "colors = spark.createDataFrame(\n",
    "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
    "    [\"id\", \"color\"])\n",
    "\n",
    "colors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n",
    "indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "indexed = indexer.fit(colors).transform(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id| color|colorIndex|\n",
      "+---+------+----------+\n",
      "|  0|   red|       1.0|\n",
      "|  1|   red|       1.0|\n",
      "|  2|  blue|       2.0|\n",
      "|  3|yellow|       0.0|\n",
      "|  4|yellow|       0.0|\n",
      "|  5|yellow|       0.0|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the dataframe\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1| [70.0,170.0,17.0]|\n",
      "|  2| [80.0,165.0,25.0]|\n",
      "|  3|[65.0,150.0,135.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataframe and display it\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(1, Vectors.dense([70, 170, 17])),\n",
    "        (2, Vectors.dense([80, 165, 25])),\n",
    "        (3, Vectors.dense([65, 150, 135]))]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StandardScaler transformer\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transformer to the dataset\n",
    "scalerModel = scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------------------------------+\n",
      "|id |features          |scaledFeatures                                             |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254367,-0.6369487984517485]|\n",
      "|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.3202563076101752,-0.5156252177942725]|\n",
      "|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021] |\n",
      "+---+------------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the scaled data\n",
    "scaledData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exercises - Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-29 17:19:24--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/proverbs.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 846 [text/csv]\n",
      "Saving to: ‘proverbs.csv’\n",
      "\n",
      "proverbs.csv        100%[===================>]     846  --.-KB/s    in 0s      \n",
      "\n",
      "2024-08-29 17:19:26 (417 MB/s) - ‘proverbs.csv’ saved [846/846]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/proverbs.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load proverbs dataset\n",
    "textdata = spark.read.csv(\"proverbs.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |text                                         |\n",
      "+---+---------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.            |\n",
      "|2  |Do not judge a book by its cover.            |\n",
      "|3  |Actions speak louder than words.             |\n",
      "|4  |A picture is worth a thousand words.         |\n",
      "|5  |If at first you do not succeed try try again.|\n",
      "+---+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display dataframe\n",
    "textdata.show(5,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-29 17:26:41--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13891 (14K) [text/csv]\n",
      "Saving to: ‘mpg.csv.1’\n",
      "\n",
      "mpg.csv.1           100%[===================>]  13.57K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-08-29 17:26:43 (243 MB/s) - ‘mpg.csv.1’ saved [13891/13891]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mpg dataset\n",
    "mpgdata = spark.read.csv(\"mpg.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|\n",
      "|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|\n",
      "|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|\n",
      "|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|\n",
      "|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display dataframe\n",
    "mpgdata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |text                                         |\n",
      "+---+---------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.            |\n",
      "|2  |Do not judge a book by its cover.            |\n",
      "|3  |Actions speak louder than words.             |\n",
      "|4  |A picture is worth a thousand words.         |\n",
      "|5  |If at first you do not succeed try try again.|\n",
      "+---+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the dataframe\n",
    "textdata.show(5,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to tokenize the \"text\" column of the \"textdata\" dataframe and store the tokens in the column \"words\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+--------------------------------------------------------+\n",
      "|id |text                                         |words                                                   |\n",
      "+---+---------------------------------------------+--------------------------------------------------------+\n",
      "|1  |When in Rome do as the Romans do.            |[when, in, rome, do, as, the, romans, do.]              |\n",
      "|2  |Do not judge a book by its cover.            |[do, not, judge, a, book, by, its, cover.]              |\n",
      "|3  |Actions speak louder than words.             |[actions, speak, louder, than, words.]                  |\n",
      "|4  |A picture is worth a thousand words.         |[a, picture, is, worth, a, thousand, words.]            |\n",
      "|5  |If at first you do not succeed try try again.|[if, at, first, you, do, not, succeed, try, try, again.]|\n",
      "+---+---------------------------------------------+--------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "textdata = tokenizer.transform(textdata)\n",
    "textdata.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Refer to Task 1\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "textdata = tokenizer.transform(textdata)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------+\n",
      "|id |words                                                   |\n",
      "+---+--------------------------------------------------------+\n",
      "|1  |[when, in, rome, do, as, the, romans, do.]              |\n",
      "|2  |[do, not, judge, a, book, by, its, cover.]              |\n",
      "|3  |[actions, speak, louder, than, words.]                  |\n",
      "|4  |[a, picture, is, worth, a, thousand, words.]            |\n",
      "|5  |[if, at, first, you, do, not, succeed, try, try, again.]|\n",
      "+---+--------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display the tokenized data\n",
    "textdata.select(\"id\",\"words\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorize the column \"words\" of the \"textdata\" dataframe and store the result in the column \"features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|                text|               words|            features|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|When in Rome do a...|[when, in, rome, ...|(99,[0,4,5,11,12,...|\n",
      "|  2|Do not judge a bo...|[do, not, judge, ...|(99,[1,3,4,19,20,...|\n",
      "|  3|Actions speak lou...|[actions, speak, ...|(99,[7,10,81,86,9...|\n",
      "|  4|A picture is wort...|[a, picture, is, ...|(99,[1,2,10,70,77...|\n",
      "|  5|If at first you d...|[if, at, first, y...|(99,[3,4,16,17,22...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol='words', outputCol='features')\n",
    "textdata = cv.fit(textdata).transform(textdata)\n",
    "textdata.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Refer to Task 2\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(textdata)\n",
    "\n",
    "textdata = model.transform(textdata)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|words                                                                   |features                                                                    |\n",
      "+------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|[when, in, rome, do, as, the, romans, do.]                              |(99,[0,4,5,11,12,41,69,93],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |\n",
      "|[do, not, judge, a, book, by, its, cover.]                              |(99,[1,3,4,19,20,31,44,54],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |\n",
      "|[actions, speak, louder, than, words.]                                  |(99,[7,10,81,86,97],[1.0,1.0,1.0,1.0,1.0])                                  |\n",
      "|[a, picture, is, worth, a, thousand, words.]                            |(99,[1,2,10,70,77,87],[2.0,1.0,1.0,1.0,1.0,1.0])                            |\n",
      "|[if, at, first, you, do, not, succeed, try, try, again.]                |(99,[3,4,16,17,22,35,53,62,64],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|[practice, makes, perfect.]                                             |(99,[24,27,34],[1.0,1.0,1.0])                                               |\n",
      "|[an, apple, a, day, keeps, the, doctor, away.]                          |(99,[0,1,13,48,57,60,63,89],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
      "|[when, the, going, gets, tough, the, tough, get, going.]                |(99,[0,6,8,11,23,42,85],[2.0,1.0,2.0,1.0,1.0,1.0,1.0])                      |\n",
      "|[all, is, fair, in, love, and, war.]                                    |(99,[2,5,37,39,61,68,96],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                     |\n",
      "|[too, many, cooks, spoil, the, broth.]                                  |(99,[0,29,33,36,40,83],[1.0,1.0,1.0,1.0,1.0,1.0])                           |\n",
      "|[you, can, not, make, an, omelette, without, breaking, eggs.]           |(99,[3,9,13,17,26,38,52,75,91],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|[the, early, bird, catches, the, worm.]                                 |(99,[0,47,51,65,73],[2.0,1.0,1.0,1.0,1.0])                                  |\n",
      "|[better, late, than, never.]                                            |(99,[7,28,46,49],[1.0,1.0,1.0,1.0])                                         |\n",
      "|[honesty, is, the, best, policy.]                                       |(99,[0,2,15,43,98],[1.0,1.0,1.0,1.0,1.0])                                   |\n",
      "|[a, penny, saved, is, a, penny, earned.]                                |(99,[1,2,14,18,74],[2.0,1.0,2.0,1.0,1.0])                                   |\n",
      "|[two, wrongs, do, not, make, a, right.]                                 |(99,[1,3,4,9,50,78,79],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |\n",
      "|[the, grass, is, always, greener, on, the, other, side, of, the, fence.]|(99,[0,2,25,55,56,71,80,88,92,95],[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[do, not, count, your, chickens, before, they're, hatched.]             |(99,[3,4,30,45,59,67,72,94],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
      "|[laughter, is, the, best, medicine.]                                    |(99,[0,2,15,32,84],[1.0,1.0,1.0,1.0,1.0])                                   |\n",
      "|[rome, wasn't, built, in, a, day.]                                      |(99,[1,5,12,21,58,76],[1.0,1.0,1.0,1.0,1.0,1.0])                            |\n",
      "+------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the resulting dataframe\n",
    "textdata.select(\"words\",\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - StringIndexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the string column \"Origin\" to a numeric column \"OriginIndex\" in the dataframe \"mpgdata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='Origin', outputCol='OriginIndex')\n",
    "indexed = indexer.fit(mpgdata).transform(mpgdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Refer to Task 5\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Origin\", outputCol=\"OriginIndex\")\n",
    "indexed = indexer.fit(mpgdata).transform(mpgdata)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|OriginIndex|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "|15.0|        6|      258.0|       110|  3730|      19.0|  75|American|        0.0|\n",
      "|27.5|        4|      134.0|        95|  2560|      14.2|  78|Japanese|        1.0|\n",
      "|26.0|        4|      108.0|        93|  2391|      15.5|  74|Japanese|        1.0|\n",
      "|20.0|        6|      156.0|       122|  2807|      13.5|  73|Japanese|        1.0|\n",
      "|29.8|        4|       89.0|        62|  1845|      15.3|  80|European|        2.0|\n",
      "|16.2|        6|      163.0|       133|  3410|      15.8|  78|European|        2.0|\n",
      "|36.0|        4|      107.0|        75|  2205|      14.5|  82|Japanese|        1.0|\n",
      "|23.0|        4|      115.0|        95|  2694|      15.0|  75|European|        2.0|\n",
      "|18.0|        6|      258.0|       110|  2962|      13.5|  71|American|        0.0|\n",
      "|14.0|        8|      350.0|       165|  4209|      12.0|  71|American|        0.0|\n",
      "|16.5|        8|      351.0|       138|  3955|      13.2|  79|American|        0.0|\n",
      "|31.0|        4|       71.0|        65|  1773|      19.0|  71|Japanese|        1.0|\n",
      "|34.1|        4|       91.0|        68|  1985|      16.0|  81|Japanese|        1.0|\n",
      "|28.0|        4|      112.0|        88|  2605|      19.6|  82|American|        0.0|\n",
      "|25.4|        6|      168.0|       116|  2900|      12.6|  81|Japanese|        1.0|\n",
      "|26.0|        4|      122.0|        80|  2451|      16.5|  74|American|        0.0|\n",
      "|19.0|        6|      225.0|       100|  3630|      17.7|  77|American|        0.0|\n",
      "|29.0|        4|       97.0|        75|  2171|      16.0|  75|Japanese|        1.0|\n",
      "|38.0|        4|       91.0|        67|  1995|      16.2|  82|Japanese|        1.0|\n",
      "|18.5|        6|      250.0|        98|  3525|      19.0|  77|American|        0.0|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the dataframe\n",
    "\n",
    "indexed.orderBy(rand()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single column named \"feaures\" using the columns \"Cylinders\", \"Engine Disp\", \"Horsepower\", \"Weight\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|MPG |features                |\n",
      "+----+------------------------+\n",
      "|15.0|[8.0,390.0,190.0,3850.0]|\n",
      "|21.0|[6.0,199.0,90.0,2648.0] |\n",
      "|18.0|[6.0,199.0,97.0,2774.0] |\n",
      "|16.0|[8.0,304.0,150.0,3433.0]|\n",
      "|14.0|[8.0,455.0,225.0,3086.0]|\n",
      "|15.0|[8.0,350.0,165.0,3693.0]|\n",
      "|18.0|[8.0,307.0,130.0,3504.0]|\n",
      "|14.0|[8.0,454.0,220.0,4354.0]|\n",
      "|15.0|[8.0,400.0,150.0,3761.0]|\n",
      "|10.0|[8.0,307.0,200.0,4376.0]|\n",
      "|15.0|[8.0,383.0,170.0,3563.0]|\n",
      "|11.0|[8.0,318.0,210.0,4382.0]|\n",
      "|10.0|[8.0,360.0,215.0,4615.0]|\n",
      "|15.0|[8.0,429.0,198.0,4341.0]|\n",
      "|21.0|[6.0,200.0,85.0,2587.0] |\n",
      "|17.0|[8.0,302.0,140.0,3449.0]|\n",
      "|9.0 |[8.0,304.0,193.0,4732.0]|\n",
      "|14.0|[8.0,340.0,160.0,3609.0]|\n",
      "|22.0|[6.0,198.0,95.0,2833.0] |\n",
      "|14.0|[8.0,440.0,215.0,4312.0]|\n",
      "+----+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Cylinders\", \"Engine Disp\", \"Horsepower\", \"Weight\"], outputCol=\"features\")\n",
    "\n",
    "mpg_transformed_data = assembler.transform(mpgdata)\n",
    "\n",
    "#show the dataframe\n",
    "mpg_transformed_data.select(\"MPG\",\"features\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use StandardScaler to scale the \"features\" column of the dataframe \"mpg_transformed_data\" and save the scaled data into the \"scaledFeatures\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "sc = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "scaledData = sc.fit(mpg_transformed_data).transform(mpg_transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "Refer to Task 6\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "\n",
    "scalerModel = scaler.fit(mpg_transformed_data)\n",
    "\n",
    "scaledData = scalerModel.transform(mpg_transformed_data)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------------------------------------------------+\n",
      "|features                |scaledFeatures                                                             |\n",
      "+------------------------+---------------------------------------------------------------------------+\n",
      "|[8.0,390.0,190.0,3850.0]|[4.689927639954407,3.7269216145389974,4.936198346102636,4.532597594013996] |\n",
      "|[6.0,199.0,90.0,2648.0] |[3.517445729965805,1.9016856443416936,2.338199216574933,3.1174853062205354]|\n",
      "|[6.0,199.0,97.0,2774.0] |[3.517445729965805,1.9016856443416936,2.520059155641872,3.265824863842812] |\n",
      "|[8.0,304.0,150.0,3433.0]|[4.689927639954407,2.9050876174868083,3.896998694291555,4.041664296168844] |\n",
      "|[8.0,455.0,225.0,3086.0]|[4.689927639954407,4.3480752169621635,5.845498041437333,3.6331418636694006]|\n",
      "|[8.0,350.0,165.0,3693.0]|[4.689927639954407,3.344673243817049,4.2866985637207105,4.347761796024335] |\n",
      "|[8.0,307.0,130.0,3504.0]|[4.689927639954407,2.9337562452909545,3.377398868386014,4.12525245959092]  |\n",
      "|[8.0,454.0,220.0,4354.0]|[4.689927639954407,4.3385190076941145,5.715598084960948,5.125955824503101] |\n",
      "|[8.0,400.0,150.0,3761.0]|[4.689927639954407,3.8224837072194844,3.896998694291555,4.427818065217309] |\n",
      "|[8.0,307.0,200.0,4376.0]|[4.689927639954407,2.9337562452909545,5.195998259055407,5.151856382183181] |\n",
      "|[8.0,383.0,170.0,3563.0]|[4.689927639954407,3.6600281496626565,4.416598520197096,4.194713046096589] |\n",
      "|[8.0,318.0,210.0,4382.0]|[4.689927639954407,3.03887454723949,5.455798172008177,5.158920170641385]   |\n",
      "|[8.0,360.0,215.0,4615.0]|[4.689927639954407,3.440235336497536,5.585698128484562,5.433230622434959]  |\n",
      "|[8.0,429.0,198.0,4341.0]|[4.689927639954407,4.099613775992897,5.144038276464853,5.110650949510326]  |\n",
      "|[6.0,200.0,85.0,2587.0] |[3.517445729965805,1.9112418536097422,2.208299260098548,3.045670123562132] |\n",
      "|[8.0,302.0,140.0,3449.0]|[4.689927639954407,2.8859751989507108,3.6371987813387845,4.06050106539072] |\n",
      "|[8.0,304.0,193.0,4732.0]|[4.689927639954407,2.9050876174868083,5.014138319988468,5.57097449736993]  |\n",
      "|[8.0,340.0,160.0,3609.0]|[4.689927639954407,3.249111151136562,4.156798607244325,4.2488687576094835] |\n",
      "|[6.0,198.0,95.0,2833.0] |[3.517445729965805,1.8921294350736448,2.468099173051318,3.3352854503484806]|\n",
      "|[8.0,440.0,215.0,4312.0]|[4.689927639954407,4.204732077941433,5.585698128484562,5.076509305295676]  |\n",
      "+------------------------+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the scaled data\n",
    "scaledData.select(\"features\",\"scaledFeatures\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have completed this lab.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ramesh Sannareddy](https://www.linkedin.com/in/rsannareddy/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork866-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-14|0.1|Ramesh Sannareddy|Initial Version Created|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
